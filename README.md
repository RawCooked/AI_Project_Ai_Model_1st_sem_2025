# ğŸ™ï¸ AI Voice Agent & Prescription Assistant

A local, privacy-first voice assistant that listens, understands, uses tools (search, Wikipedia, math, notes, files), and can generate modern prescription PDFs. Built around Whisper for speech-to-text and a configurable LLM backend.

â€”

## âœ¨ Highlights
- ğŸ§ Voice capture with automatic Voice Activity Detection (hands-free)
- ğŸ§  On-device transcription via Whisper
- ğŸ¤ Tool-aware agent: web search, Wikipedia, math, notes, file save
- ğŸ’Š Medical helpers: quick prescriptions for common conditions + full PDF generator
- ğŸ”’ Local-first workflow; configurable LLM endpoint (no public keys required)
- ğŸ—‚ï¸ Conversation logging to JSON with tool call history

â€”

## ğŸ“¦ Tech Stack
- Python 3.10+
- Whisper (openai-whisper)
- SoundDevice (mic input)
- ReportLab (PDF generation)
- OpenAI-compatible Chat Completions client (points to your school/hosted LLM)
- httpx (client)

â€”

## ğŸ“ Project Structure
- README.md â€” this file
- Requirements.txt â€” Python dependencies
- voice_agent_system.py â€” main app (run this)
- env/ â€” optional local venv (if you created one)

â€”

## ğŸ§¾ Requirements
Create a virtual environment (recommended):

```
python -m venv env
./env/Scripts/activate  # Windows
# source env/bin/activate  # macOS/Linux
```

Install dependencies:
```
pip install -r Requirements.txt
```

If you plan to use the PDF prescription feature, ReportLab will be installed from Requirements.txt. Microphone support uses sounddevice (no extra driver on most systems).

â€”

## ğŸ” Configure LLM Endpoint (.env)
This project talks to a hosted, OpenAI-compatible LLM (school/organization endpoint). Create a .env file in the project root:

```
ESPRIT_API_KEY=your_api_key_here
ESPRIT_BASE_URL=https://tokenfactory.esprit.tn/api/v1
LLM_MODEL=hosted_vllm/Llama-3.1-70B-Instruct
```

Notes:
- ESPRIT_BASE_URL can be changed to any OpenAI-compatible endpoint.
- LLM_MODEL is the model name exposed by your endpoint.

â€”

## ğŸš€ Run
Start the application:
```
python voice_agent_system.py
```
Then speak after the â€œListeningâ€¦â€ prompt. The app auto-stops when it detects a short silence and processes your request.

â€”

## ğŸ—£ï¸ Example Voice Prompts
- â€œMy name is Sarah and I have a headacheâ€
- â€œWhat time is it?â€
- â€œSearch for the latest AI newsâ€
- â€œCreate a prescription for John with coldâ€

â€”

## ğŸ§ª Tools You Can Ask For
- search_web â€” quick web snippets
- get_wikipedia_info â€” short summary + page URL
- calculate â€” basic calculator
- get_current_datetime â€” date/time
- create_note â€” saves a note file
- save_to_file â€” writes any content to a .txt file
- create_prescription â€” full PDF with patient/meds/notes
- quick_prescription â€” fast preset for common conditions

â€”

## âš•ï¸ Prescription Safety
- The agent asks for the patient name before generating medical outputs.
- Output is for informational/educational purposes only. Always consult a licensed physician.

â€”

## âš™ï¸ Config Knobs (voice and whisper)
You can adjust these in Config inside voice_agent_system.py:
- SAMPLE_RATE, SILENCE_THRESHOLD, SILENCE_DURATION
- MAX_RECORDING_TIME
- WHISPER_MODEL (e.g., "tiny", "base", â€¦)

â€”

## ğŸ§© Troubleshooting
- No mic detected: ensure system microphone permissions are granted for your terminal.
- SSL issues when calling your endpoint: the client is configured to skip certificate verification; consider providing a valid cert in production.
- Whisper model too slow on your machine: switch to a smaller WHISPER_MODEL (e.g., tiny, base).

â€”

## ğŸ“œ License
Educational/demonstration use. See repository terms or contact the author for details.
